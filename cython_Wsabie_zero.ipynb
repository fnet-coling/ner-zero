{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "from random import randint\n",
    "from data_load import *\n",
    "from cwsabie_inner import *\n",
    "from transE_label import *\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Wsabie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "from TypeHierarchy import *\n",
    "cimport numpy as np\n",
    "from random import randint\n",
    "import sys\n",
    "import cython\n",
    "cdef extern from \"math.h\":\n",
    "    double sqrt(double m)\n",
    "import math\n",
    "from libc.stdlib cimport malloc, free\n",
    "\n",
    "from libc.math cimport exp\n",
    "from libc.math cimport log\n",
    "from gensim.matutils import argsort\n",
    "\n",
    "from libc.string cimport memset\n",
    "import random\n",
    "# scipy <= 0.15\n",
    "\n",
    "import scipy.linalg.blas as fblas\n",
    "ctypedef np.float32_t REAL_t\n",
    "cdef int ONE = 1\n",
    "\n",
    "\n",
    "REAL = np.float32\n",
    "cdef extern from \"/Users/mayk/working/figer/baseline/PLE/Model/warp/voidptr.h\":\n",
    "    void* PyCObject_AsVoidPtr(object obj)\n",
    "DEF MAX_SENTENCE_LEN = 10000\n",
    "ctypedef void (*scopy_ptr) (const int *N, const float *X, const int *incX, float *Y, const int *incY) nogil\n",
    "ctypedef void (*saxpy_ptr) (const int *N, const float *alpha, const float *X, const int *incX, float *Y, const int *incY) nogil\n",
    "ctypedef float (*sdot_ptr) (const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil\n",
    "ctypedef double (*dsdot_ptr) (const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil\n",
    "ctypedef double (*snrm2_ptr) (const int *N, const float *X, const int *incX) nogil\n",
    "ctypedef void (*sscal_ptr) (const int *N, const float *alpha, const float *X, const int *incX) nogil\n",
    "\n",
    "\n",
    "cdef scopy_ptr scopy = <scopy_ptr>PyCObject_AsVoidPtr(fblas.scopy._cpointer)  # y = x\n",
    "cdef saxpy_ptr saxpy=<saxpy_ptr>PyCObject_AsVoidPtr(fblas.saxpy._cpointer)  # y += alpha * x\n",
    "cdef sdot_ptr sdot=<sdot_ptr>PyCObject_AsVoidPtr(fblas.sdot._cpointer)  # float = dot(x, y)\n",
    "cdef dsdot_ptr dsdot=<dsdot_ptr>PyCObject_AsVoidPtr(fblas.sdot._cpointer)  # double = dot(x, y)\n",
    "cdef snrm2_ptr snrm2=<snrm2_ptr>PyCObject_AsVoidPtr(fblas.snrm2._cpointer)  # sqrt(x^2)\n",
    "cdef sscal_ptr sscal=<sscal_ptr>PyCObject_AsVoidPtr(fblas.sscal._cpointer) # x = alpha * x\n",
    "DEF EXP_TABLE_SIZE = 10000\n",
    "DEF MAX_EXP = 50\n",
    "\n",
    "cdef REAL_t[EXP_TABLE_SIZE] EXP_TABLE\n",
    "cdef REAL_t[EXP_TABLE_SIZE] LOG_TABLE\n",
    "\n",
    "cdef REAL_t ONEF = <REAL_t>1.0\n",
    "\n",
    "# for when fblas.sdot returns a double\n",
    "cdef REAL_t our_dot_double(const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil:\n",
    "    return <REAL_t>dsdot(N, X, incX, Y, incY)\n",
    "\n",
    "# for when fblas.sdot returns a float\n",
    "cdef REAL_t our_dot_float(const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil:\n",
    "    return <REAL_t>sdot(N, X, incX, Y, incY)\n",
    "\n",
    "# for when no blas available\n",
    "cdef REAL_t our_dot_noblas(const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil:\n",
    "    # not a true full dot()-implementation: just enough for our cases\n",
    "    cdef int i\n",
    "    cdef REAL_t a\n",
    "    a = <REAL_t>0.0\n",
    "    for i from 0 <= i < 50 by 1:\n",
    "        a += X[i] * Y[i]\n",
    "    return a\n",
    "\n",
    "# for when no blas available\n",
    "cdef void our_saxpy_noblas(const int *N, const float *alpha, const float *X, const int *incX, float *Y, const int *incY) nogil:\n",
    "    cdef int i\n",
    "    for i from 0 <= i < N[0] by 1:\n",
    "        Y[i * (incY[0])] = (alpha[0]) * X[i * (incX[0])] + Y[i * (incY[0])]\n",
    "cdef REAL_t cvdot(vec1,vec2,size):\n",
    "    cdef int csize = size\n",
    "    f= dsdot(&csize,<REAL_t *>(np.PyArray_DATA(vec1)),&ONE,<REAL_t *>(np.PyArray_DATA(vec2)),&ONE)\n",
    "    return f\n",
    "def csaxpy(vec1,vec2,alpha,size):\n",
    "    cdef int csize = size\n",
    "    cdef float calpha = alpha\n",
    "    f= our_saxpy_noblas(&csize,&calpha,<REAL_t *>(np.PyArray_DATA(vec1)),&ONE,<REAL_t *>(np.PyArray_DATA(vec2)),&ONE)\n",
    "    return f\n",
    "cdef REAL_t crank(int k):\n",
    "    cdef REAL_t loss = 0.\n",
    "    cdef int i = 1\n",
    "    for i in range(1,k+1):\n",
    "        loss += ONEF/i\n",
    "    return loss\n",
    "cdef REAL_t rev_crank(int k):\n",
    "    cdef REAL_t loss = 2.\n",
    "    cdef int i = 1\n",
    "    for i in range(1,k+1):\n",
    "        loss -= ONEF/i\n",
    "    return loss\n",
    "cdef REAL_t vsum(REAL_t *vec,int *size):\n",
    "    cdef int i\n",
    "    cdef REAL_t product\n",
    "    product = <REAL_t>0.0\n",
    "    for i from 0 <= i < size[0] by 1:\n",
    "        product += vec[i] **2\n",
    "    return sqrt(product)\n",
    "def cnorm(vec):\n",
    "    cdef int size\n",
    "    size  = len(vec)\n",
    "    return vsum(<REAL_t *>(np.PyArray_DATA(vec)),&size)\n",
    "def init():\n",
    "    for i in range(EXP_TABLE_SIZE):\n",
    "        EXP_TABLE[i] = <REAL_t>exp((i / <REAL_t>EXP_TABLE_SIZE * 2 - 1) * MAX_EXP)\n",
    "        EXP_TABLE[i] = <REAL_t>(EXP_TABLE[i] / (EXP_TABLE[i] + 1))\n",
    "#init()\n",
    "\n",
    "\n",
    "def ctrain(A,B,insts,size,lr,gradient,it,Verbose=False):\n",
    "    cdef float error\n",
    "    next_random = 1\n",
    "    error = 0.\n",
    "   # cdef lam = 0.01\n",
    "    for i,inst in enumerate(insts):\n",
    "        err,next_random =gradient(A,B,inst,size,next_random,lr=lr)\n",
    "      \n",
    "        error += err\n",
    "        if i % 1000 ==0 and Verbose:\n",
    "            sys.stdout.write(\"\\rIteration %d \" % (it)+ \"trained {0:.0f}%\".format(float(i)*100/len(insts))+\" Loss:{0:.2f}\".format(error))\n",
    "            sys.stdout.flush()\n",
    "    if Verbose:\n",
    "        sys.stdout.write(\"\\n\")\n",
    "    return error\n",
    "\n",
    "cdef void divide(REAL_t *vec, const float *alpha, const int *size):\n",
    "    cdef int i\n",
    "    for i from 0 <= i < size[0] by 1:\n",
    "        vec[i] = vec[i]/alpha[0]\n",
    "def cdivide(vec,alpha):\n",
    "    cdef int size\n",
    "    size  = len(vec)\n",
    "    cdef float r = alpha\n",
    "    divide(<REAL_t *>(np.PyArray_DATA(vec)),&r,&size)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def warp_gradient(A,B,inst,size,next_random,lr=0.01):\n",
    "    #print B\n",
    "    #print B[0]-B[9]\n",
    "    cdef unsigned long long  c_next_random = next_random\n",
    "    dA = np.zeros(size,dtype=REAL)\n",
    "    dB = np.zeros(B.shape,dtype=REAL)\n",
    "\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "    cdef REAL_t error = 0.\n",
    "    cdef REAL_t clr = lr\n",
    "    cdef int N,n_sample \n",
    "    cdef int neg_num = len(inst.negative_labels)\n",
    "    cdef REAL_t norm\n",
    "    cdef int cSize = size\n",
    "    cdef REAL_t floats\n",
    "    M = len(inst.sparse_labels)\n",
    "    for l in inst.sparse_labels:\n",
    "        s1 = cvdot(x,B[l],cSize)\n",
    "        N = 1\n",
    "        n_sample  = -1\n",
    "        for k in range(neg_num):\n",
    "            c_next_random = random_int32(&c_next_random)\n",
    "            nl = inst.negative_labels[c_next_random%len(inst.negative_labels)]#randint(0,neg_num-1)]\n",
    "            s2 = cvdot(x,B[nl],cSize)\n",
    "            if s1 - s2<1:\n",
    "                n_sample = nl\n",
    "                N = k+1\n",
    "                break\n",
    "        if n_sample!=-1:\n",
    "            L = crank(len(inst.negative_labels)/N)#+rev_crank(i)\n",
    "            negL = -L\n",
    "            error += (1+s2-s1)*L\n",
    "            csaxpy(B[l]-B[n_sample],dA,L,cSize)\n",
    "\n",
    "#             csaxpy(x,dB[l],L,cSize)\n",
    "#             csaxpy(x,dB[n_sample],-L,cSize)\n",
    "    for f in inst.features:\n",
    "        csaxpy(dA,A[f],clr,cSize)\n",
    "        norm = cnorm(A[f])\n",
    "        if norm >1:\n",
    "            cdivide(A[f],norm)\n",
    "#     for i in range(len(B)):\n",
    "#         csaxpy(dB[i],B[i],clr,cSize)\n",
    "#         #B[i] += lr*dB[i]\n",
    "#         norm =  cnorm(B[i])\n",
    "#         if norm >1:\n",
    "#             cdivide(B[i],norm)\n",
    "#             B[i] /=norm\n",
    "    return error,c_next_random\n",
    "\n",
    "\n",
    "def save_to_text(matrix,output):\n",
    "    shape = matrix.shape\n",
    "    with open(output,'wb') as out:\n",
    "        out.write(\"%d %d\\n\" % (shape))\n",
    "        for row in matrix:\n",
    "            x = \" \".join(map(lambda x:\"{0:.5}\".format(x),row))\n",
    "            out.write(x+\"\\n\")\n",
    "\n",
    "cdef inline unsigned long long random_int32(unsigned long long *next_random) nogil:\n",
    "    next_random[0] = (next_random[0] * <unsigned long long>25214903917ULL + 11) & 281474976710655ULL\n",
    "    return next_random[0]\n",
    "def crand(sed):\n",
    "    cdef unsigned long long csed = sed\n",
    "    return random_int32(&csed)\n",
    "def save2bin(mat,dct,fn):\n",
    "    n,d  = mat.shape\n",
    "    with open(fn,'w') as out:\n",
    "        out.write(\"%d %d\\n\" % (n,d))\n",
    "        for i in range(n):\n",
    "            text = \" \".join(map(str,mat[i]))\n",
    "            out.write(\"%s %s\\n\" %(dct[i],text))\n",
    "def normalize(mat):\n",
    "    for v in mat:\n",
    "        norm = np.linalg.norm(v)\n",
    "        if norm >=1:\n",
    "            v /= norm\n",
    "def refine(labels,type_hierarchy,maxDepth=2,delim='/'):\n",
    "    results = []\n",
    "    labels = [labels[i] for i in range(len(labels))]\n",
    "    pathes = [ type_hierarchy.get_type_path(labels[i]) for i in range(len(labels))]\n",
    "    for i in range(len(labels)):\n",
    "        for k in range(len(pathes[i])):\n",
    "            if len(results)<k+1:\n",
    "                results.append(pathes[i][k])\n",
    "            elif results[-1]!=pathes[i][k]:\n",
    "                break\n",
    "    return results\n",
    "\n",
    "def refine_data(A,B,data,tier):\n",
    "    for inst in data:\n",
    "        inst.sparse_labels = test(A,B,tier,inst)\n",
    "\n",
    "\n",
    "def test(A,B,tier,inst,topk=3):\n",
    "    cdef int cSize = B.shape[1]\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "\n",
    "    scores = [cvdot(x,B[l],cSize) for l in inst.old_labels]\n",
    "    ranks = argsort(scores,reverse=True)\n",
    "    return refine([inst.old_labels[r] for r in ranks],tier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_dir= \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN\"\n",
    "a=MentionData('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_x_new.txt',\n",
    "              \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_y.txt\",\n",
    "             in_dir+\"/feature.txt\",in_dir+\"/type.txt\")\n",
    "label_bin  = Word2Vec.load_word2vec_format('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/labels.bin')\n",
    "label_bin_hier  = Word2Vec.load_word2vec_format('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/labels_hier.bin')\n",
    "#label_bin  = Word2Vec.load_word2vec_format('/Users/mayk/working/figer/baseline/PLE/Model/warp/embedding/BBN/inter_l2v_300.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 trained 100% Loss:156172.92\n",
      "Iteration 1 trained 100% Loss:105183.62\n",
      "Iteration 2 trained 100% Loss:91517.77\n",
      "Iteration 3 trained 100% Loss:83816.83\n",
      "Iteration 4 trained 100% Loss:78126.23\n",
      "Iteration 5 trained 100% Loss:74125.27\n",
      "Iteration 6 trained 100% Loss:71152.59\n",
      "Iteration 7 trained 100% Loss:68320.26\n",
      "Iteration 8 trained 100% Loss:65867.26\n",
      "Iteration 9 trained 100% Loss:63805.31\n",
      "Iteration 10 trained 100% Loss:62444.55\n",
      "Iteration 11 trained 100% Loss:60544.55\n",
      "Iteration 12 trained 100% Loss:59716.10\n",
      "Iteration 13 trained 100% Loss:58010.41\n",
      "Iteration 14 trained 100% Loss:57262.91\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(12)\n",
    "size= 300\n",
    "#A= np.random.normal(0,1, [len(a.feature2id),size]).astype(np.float32)#np.random.rand(len(a.feature2id),size).astype(np.float32)\n",
    "A= np.random.uniform(-6/np.sqrt(size),6/np.sqrt(size), [len(a.feature2id),size]).astype(np.float32)#np.random.rand(len(a.feature2id),size).astype(np.float32)\n",
    "B =np.asarray([label_bin[a.id2label[i]] for i in range(len(label_bin.vocab))],dtype=np.float32)#np.random.normal(0,1.0, [len(a.label2id),size]).astype(np.float32)\n",
    "next_random = 1\n",
    "normalize(A)\n",
    "normalize(B)\n",
    "for i in range(15): \n",
    "    ctrain(A,B,a.data,size,0.001,warp_gradient,it=i,Verbose=True)\n",
    "\n",
    "save2bin(A,a.id2feature,'/Users/mayk/working/figer/baseline/PLE/Results/BBN/warp_A.bin')\n",
    "save2bin(B,a.id2label,'/Users/mayk/working/figer/baseline/PLE/Results/BBN/warp_B.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save2bin(A,a.id2feature,'/Users/mayk/working/figer/baseline/PLE/Results/warp_A.bin')\n",
    "save2bin(B,a.id2label,'/Users/mayk/working/figer/baseline/PLE/Results/warp_B.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_tid(fn,func=int,delim=\" \"):\n",
    "    with open(fn,'r') as types:\n",
    "        tid = dict()\n",
    "        for ln in types:\n",
    "            split = ln.rstrip().split(delim)\n",
    "            tid[split[0]] = func(split[1])\n",
    "    return tid\n",
    "tid = read_tid('../../Intermediate/BBN/mention.txt',delim='\\t')\n",
    "wh=[tid[mid] for mid,name in  read_tid('../../Intermediate/BBN/mention_dict.txt',str,':').iteritems() if name == \"White House\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../../Intermediate/BBN/relation.txt','r') as readin:\n",
    "    for ln in readin:\n",
    "        splited = ln.rstrip().split()\n",
    "        right = int(splited[0])\n",
    "        left = int(splited[1])\n",
    "        if len(splited) < 3:\n",
    "            a.data[right].sparse_labels = []\n",
    "            a.data[left].sparse_labels = []\n",
    "        else:\n",
    "            labels = map(lambda x:a.label2id[x],splited[2:])\n",
    "            full_labels = np.zeros(len(a.id2label),dtype=float)\n",
    "            full_labels[labels]=1.\n",
    "            a.data[right]=Instance(a.data[right].features,full_labels,labels)\n",
    "            a.data[left]=Instance(a.data[left].features,full_labels,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in wh:\n",
    "    labels = map(lambda x:a.label2id[x],['/ORGANIZATION','/ORGANIZATION/GOVERNMENT'])\n",
    "    full_labels = np.zeros(len(a.id2label),dtype=float)\n",
    "    full_labels[labels]=1.\n",
    "    a.data[i]=Instance(a.data[i].features,full_labels,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def filte_rare(data,labels):\n",
    "    \n",
    "    new_data = []\n",
    "    ids = []\n",
    "    for i in range(len(data)):\n",
    "        skip = False\n",
    "        for l in data[i].sparse_labels:\n",
    "            if l in labels:\n",
    "                skip =True\n",
    "                break\n",
    "        if not skip:\n",
    "            ids.append(i)\n",
    "            new_data.append(data[i])\n",
    "    return ids,new_data\n",
    "def keep_rare(data,labels):\n",
    "    \n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        skip = False\n",
    "        for l in data[i].sparse_labels:\n",
    "            if l in labels:\n",
    "                skip =True\n",
    "                break\n",
    "        if skip:\n",
    "            new_data.append(data[i])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ANIMAL\n",
      "/GAME\n",
      "/ORGANIZATION/HOSPITAL\n",
      "/FACILITY/ATTRACTION\n",
      "/PRODUCT/WEAPON\n",
      "/FACILITY/HIGHWAY_STREET\n",
      "/LOCATION/CONTINENT\n",
      "/ORGANIZATION/MUSEUM\n",
      "/FACILITY/BUILDING\n",
      "/FACILITY/BRIDGE\n",
      "/WORK_OF_ART/PLAY\n",
      "/LAW\n",
      "/EVENT/HURRICANE\n",
      "/PRODUCT/VEHICLE\n",
      "/LOCATION/RIVER\n",
      "/FACILITY/AIRPORT\n",
      "/ORGANIZATION/HOTEL\n",
      "/ORGANIZATION/RELIGIOUS\n",
      "/LOCATION/LAKE_SEA_OCEAN\n"
     ]
    }
   ],
   "source": [
    "rare_labels = set()\n",
    "d = [0]*len(a.label2id)\n",
    "for inst in a.data:\n",
    "    for l in inst.sparse_labels:\n",
    "        d[l]+=1\n",
    "for i in range(len(d)):\n",
    "    if d[i] < 500:\n",
    "        print a.id2label[i]\n",
    "        rare_labels.add(i)\n",
    "ids,a.data= filte_rare(a.data,rare_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir= \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN\"\n",
    "test=MentionData('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/test_x.txt',\n",
    "              \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/test_y.txt\",\n",
    "             in_dir+\"/feature.txt\",in_dir+\"/type.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rare_labels = set()\n",
    "d = [0]*len(a.label2id)\n",
    "for inst in a.data:\n",
    "    for l in inst.sparse_labels:\n",
    "        d[l]+=1\n",
    "for i in range(len(d)):\n",
    "    if d[i] < 5000:\n",
    "        rare_labels.add(i)\n",
    "with open('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/mention.txt') as mentions:\n",
    "    mention_dict = dict()\n",
    "    for ln in mentions:\n",
    "        splited = ln.rstrip().split()\n",
    "        mention_dict[splited[0]] = splited[1]\n",
    "ids,test.data = filte_rare(test.data,[l for l in range(len(test.label2id)) if l not in rare_labels])\n",
    "origin_ids = [ ln.rstrip().split()[0] for ln in open('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/test_x.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/mention_type_zero.txt','w') as rare,open('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/test_x_zero.txt','w') as x:\n",
    "    for i,inst in enumerate(test.data):\n",
    "        x.write(\"%s\\t%s\\n\" % (origin_ids[ids[i]],\",\".join(map(str,inst.features))))\n",
    "        for l in inst.sparse_labels:\n",
    "            rare.write(\"%s\\t%d\\t%d\\n\" %(mention_dict[origin_ids[ids[i]]],l,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tier= TypeHierarchy('../../Intermediate/BBN/supertype.txt',B.shape[0])\n",
    "for inst in a.data:\n",
    "    labels = set()\n",
    "    for l in inst.sparse_labels:\n",
    "        path= tier.get_type_path(l)\n",
    "        labels.add(path[0])\n",
    "    labels = list(labels)\n",
    "    inst.sparse_labels = labels\n",
    "#     full_labels = np.zeros(len(a.id2label),dtype=float)\n",
    "#     full_labels[labels]=1.\n",
    "#     a.data[i]=Instance(a.data[i].features,full_labels,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/PERSON']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate = lambda x: map(lambda y:a.id2label[y],x)\n",
    "translate(a.data[1009].sparse_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_bin_hier['/LOCATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 17]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b= map(lambda x:a.label2id[x],['/GPE','/GPE/CITY','/PERSON'])\n",
    "refine(b,tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 17, 19]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
