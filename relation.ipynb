{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from mention import *\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(deps):\n",
    "    return [[d['gov'],d['dep']] for d in deps]\n",
    "def matched(entity,nodes):\n",
    "    for i in range(entity.start,entity.end):\n",
    "        if i in nodes:return i\n",
    "def shortestPath(e1,e2,graph,nodes):\n",
    "    try:\n",
    "        path= nx.shortest_path(graph, source=matched(e1,nodes),target=matched(e2,nodes))\n",
    "    except:\n",
    "        return []\n",
    "    shortest = []\n",
    "    for i in path:\n",
    "        if e1.start<=i <e1.end:\n",
    "            shortest = [i]\n",
    "        elif i>=e2.start:\n",
    "            shortest.append(i)\n",
    "            return shortest\n",
    "        else:\n",
    "            shortest.append(i)\n",
    "    return shortest\n",
    "def stringPath(path,tokens,deps):\n",
    "    res = []\n",
    "    for i in range(len(path)-1):\n",
    "        for d in deps:\n",
    "            if d['gov'] == path[i] and d['dep'] ==path[i+1]:\n",
    "       \n",
    "                splited=d[\"type\"].split(\"_\",1)\n",
    "                \n",
    "                res.append(\":<\"+splited[0]+\">:\")\n",
    "                for t in splited[1:]:\n",
    "                    res.append(t+\":\")\n",
    "                \n",
    "                if i!=len(path)-2:\n",
    "                    res[-1] += tokens[path[i+1]]\n",
    "                break\n",
    "            elif d['dep'] == path[i] and d['gov'] ==path[i+1]:\n",
    "                d['type']=\"-\"+d[\"type\"]\n",
    "                splited=d[\"type\"].split(\"_\",1)\n",
    "                \n",
    "                res.append(\":<\"+splited[0]+\">:\")\n",
    "                for t in splited[1:]:\n",
    "                    res.append(t+\":\")\n",
    "                \n",
    "               # res.append(\":\"+d['type']+\":\")\n",
    "                if i!=len(path)-2:\n",
    "                    res[-1] += tokens[path[i+1]]\n",
    "                break\n",
    "    return \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/test_new.json'\n",
    "with open(filename,'r') as f,open('./patterns','w') as patterns:\n",
    "        for line in f:\n",
    "            sent = json.loads(line.strip('\\r\\n'))\n",
    "            mentions = [Mention(int(m['start']), int(m['end']), m['labels'],\" \".join(sent['tokens'][m['start']:m['end']])) for m in sent['mentions']]\n",
    "            if len(mentions)>1:\n",
    "                x = graph(sent['dep'])\n",
    "                nodes = set([l for n in x for l in n])\n",
    "\n",
    "                g = nx.Graph(x)\n",
    "                for i in range(len(mentions)-1):\n",
    "                    \n",
    "                    link= shortestPath(mentions[i],mentions[i+1],g,nodes)\n",
    "                    if len(link)<=2 and len(link)>=2:\n",
    "                      #  patterns.write(\"#\".join(mentions[i].labels)+stringPath(link,sent['tokens'],sent['dep'])+\"#\".join(mentions[i+1].labels)+\"\\n\")\n",
    "                        patterns.write(stringPath(link,sent['tokens'],sent['dep'])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_new.json'\n",
    "relation_id = dict()\n",
    "label_id = dict()\n",
    "with open(filename,'r') as f,open('./relation2id.txt','w') as rid,open('./entity2id.txt','w') as lid,open('./train.txt','w') as out:\n",
    "        for line in f:\n",
    "            sent = json.loads(line.strip('\\r\\n'))\n",
    "            mentions = [Mention(int(m['start']), int(m['end']), m['labels'],\" \".join(sent['tokens'][m['start']:m['end']])) for m in sent['mentions']]\n",
    "            if len(mentions)>1:\n",
    "                x = graph(sent['dep'])\n",
    "                nodes = set([l for n in x for l in n])\n",
    "\n",
    "                g = nx.Graph(x)\n",
    "                for i in range(len(mentions)-1):\n",
    "                    \n",
    "                    link= shortestPath(mentions[i],mentions[i+1],g,nodes)\n",
    "                    if link == []:continue\n",
    "                    for l in mentions[i].labels:\n",
    "                        if l not in label_id:\n",
    "                            label_id[l] = len(label_id)\n",
    "                    for l in mentions[i+1].labels:\n",
    "                        if l not in label_id:\n",
    "                            label_id[l] = len(label_id)   \n",
    "                    if len(link)<=5 and len(link)>=2:\n",
    "                        rel = stringPath(link,sent['tokens'],sent['dep'])\n",
    "                        if len(rel) == 0:continue\n",
    "                        if rel not in relation_id:\n",
    "                            relation_id[rel]=len(relation_id)\n",
    "                        for l in mentions[i].labels:\n",
    "                            for lr in mentions[i+1].labels:\n",
    "                                out.write(\"%s\\t[XXX]%s[YYY]\\t%s\\n\" % (l,rel,lr))\n",
    "        for k,v in label_id.iteritems():\n",
    "            lid.write(\"%s\\t%d\\n\" % (k,v))\n",
    "        for k,v in relation_id.iteritems():\n",
    "            rid.write(\"%s\\t%d\\n\" % (k,v))\n",
    "        lid.close()\n",
    "        rid.close()\n",
    "                      #  patterns.write(\"#\".join(mentions[i].labels)+stringPath(link,sent['tokens'],sent['dep'])+\"#\".join(mentions[i+1].labels)+\"\\n\")\n",
    "                     #   patterns.write(stringPath(link,sent['tokens'],sent['dep'])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entities = Word2Vec.load_word2vec_format('/Users/mayk/working/rel_extractor/baseline/Relation_Extraction/TransE/label.bin',binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rels = Word2Vec.load_word2vec_format('/Users/mayk/working/rel_extractor/baseline/Relation_Extraction/TransE/rels.bin',binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "from gensim.matutils import argsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def query(query,rel,entities,rels):\n",
    "    a =rels[rel]+entities[query]\n",
    "    dists = [LA.norm(a-vec) for vec in entities.syn0]\n",
    "    best = argsort(dists)[0]\n",
    "    if entities.index2word[best]!=query: \n",
    "        print query,rel,entities.index2word[best],dists[best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in entities.vocab:\n",
    "    for r in rels.vocab:\n",
    "        query(e,r,entities,rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"conj_and\".replace('_',\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_new.json'\n",
    "relation_id = dict()\n",
    "label_id = dict()\n",
    "words = set()\n",
    "cnt = 0\n",
    "with open(filename,'r') as f,open('./relation2id.txt','w') as rid,open('./entity2id.txt','w') as lid,open('./train.txt','w') as out:\n",
    "        for line in f:\n",
    "            sent = json.loads(line.strip('\\r\\n'))\n",
    "            mentions = [Mention(int(m['start']), int(m['end']), m['labels'],\" \".join(sent['tokens'][m['start']:m['end']])) for m in sent['mentions']]\n",
    "            if len(mentions)>1:\n",
    "                x = graph(sent['dep'])\n",
    "                nodes = set([l for n in x for l in n])\n",
    "                g = nx.Graph(x)\n",
    "                for i in range(len(mentions)-1):\n",
    "                    \n",
    "                    link= shortestPath(mentions[i],mentions[i+1],g,nodes)\n",
    "                    if link == []:continue\n",
    "                    for l in mentions[i].labels:\n",
    "                        if l not in label_id:\n",
    "                            label_id[l] = len(label_id)\n",
    "                    for l in mentions[i+1].labels:\n",
    "                        if l not in label_id:\n",
    "                            label_id[l] = len(label_id)   \n",
    "                    if len(link)<=2 and len(link)>=2:\n",
    "                        for node in link:\n",
    "                            words.add(sent['tokens'][node].lower())\n",
    "                        rel = stringPath(link,sent['tokens'],sent['dep'])\n",
    "                        if len(rel) == 0:continue\n",
    "                        if rel not in relation_id:\n",
    "                            relation_id[rel]=len(relation_id)\n",
    "                        if mentions[i].name == 'It' or mentions[i+1].name == 'It' or mentions[i].name == 'The' or mentions[i+1].name == 'The':\n",
    "                            continue\n",
    "                        if mentions[i].name.lower() in model.vocab and mentions[i+1].name.lower() in model.vocab:\n",
    "                            similarity  =model.similarity(mentions[i].name.lower(),mentions[i+1].name.lower())\n",
    "                            if similarity >0.5:\n",
    "                                cnt +=1\n",
    "                                out.write(\"%s\\t[XXX]%s[YYY]\\t%s\\n\" % (\"#\".join(mentions[i].labels),rel,\"#\".join(mentions[i+1].labels)))\n",
    "\n",
    "                        #out.write(\" \".join(sent['tokens'])+\"\\n\")\n",
    "\n",
    "                        #out.write(\"%s\\t%s\\t[XXX]%s[YYY]\\t%s\\t%s\\n\" % (\"#\".join(mentions[i].labels),mentions[i].name,rel,\"#\".join(mentions[i+1].labels),mentions[i+1].name))\n",
    "        for k,v in label_id.iteritems():\n",
    "            lid.write(\"%s\\t%d\\n\" % (k,v))\n",
    "        for k,v in relation_id.iteritems():\n",
    "            rid.write(\"%s\\t%d\\n\" % (k,v))\n",
    "\n",
    "                      #  patterns.write(\"#\".join(mentions[i].labels)+stringPath(link,sent['tokens'],sent['dep'])+\"#\".join(mentions[i+1].labels)+\"\\n\")\n",
    "                     #   patterns.write(stringPath(link,sent['tokens'],sent['dep'])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Volumes/My Passport/RR_backup/data/dependency_embedding/deps.words','rb') as vectors, open('/Volumes/My Passport/RR_backup/data/dependency_embedding/shortlist','wb') as out:\n",
    "    for ln in vectors:\n",
    "        print ln\n",
    "        if ln.split()[0] in words:\n",
    "            out.write(ln)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.matutils import argsort\n",
    "from numpy import array,asarray\n",
    "import numpy as np\n",
    "model  = Word2Vec.load_word2vec_format('/Volumes/My Passport/RR_backup/data/dependency_embedding/shortlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2491"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
